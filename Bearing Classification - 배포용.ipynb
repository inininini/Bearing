{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 6. Bearing Defect Type Classification\n",
    "\n",
    "### **2019/7/1 SK Hynix - KAIST**<br/>\n",
    "<br/>\n",
    "\n",
    "***Tip> shotcuts for Jupyter Notebook***\n",
    "* Shift + Enter : run cell and select below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">베어링에는 크게 볼 손상, 내륜 손상, 외륜 손상의 3가지 결함이 있습니다.<br/>\n",
    "이번 시간에는 Deep Neural Network를 사용하여, 베어링의 정상 신호와 3가지 결함을 분류해보는 문제를 풀어보도록 하겠습니다.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img\\bearing.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img\\defect.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "실습에 필요한 module을 불러옵니다. <br/>\n",
    "\n",
    "Tensorflow : Deep learning 학습에 최적화된 module <br/>\n",
    "numpy : 수학적 연산에 관련된 module <br/>\n",
    "os : system 내부 제어에 사용되는 module\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "실습에 필요한 data를 불러오겠습니다. <br/>\n",
    "외륜 손상은 'OR', 볼 손상은 'B', 내륜 손상은 'IR', 정상은 'N'으로 나타내겠습니다. <br/>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-(1). Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "베어링 신호가 위치한 폴더에 접근하고, 코드 구현의 편의성을 위해 각 손상 class별로 분류해줍니다.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 내 파일 목록 불러오기\n",
    "file_list = os.listdir('./CWRU_360_npy')\n",
    "\n",
    "# 분류를 위한 empty list들 생성\n",
    "B_files, IR_files, OR_files, N_files = [], [], [], []\n",
    "\n",
    "# 손상 class별 파일 분류\n",
    "for item in file_list:\n",
    "    if item.find('B') is not -1 : # 해당 문자열을 이름에 포함하지 않는 경우 -1을 return함.\n",
    "        B_files.append(item)\n",
    "    if item.find('IR') is not -1 : \n",
    "        IR_files.append(item)\n",
    "    if item.find('OR') is not -1 : \n",
    "        OR_files.append(item)\n",
    "    if item.find('N') is not -1 : \n",
    "        N_files.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "각 class 별 파일의 개수를 확인해보겠습니다.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Defect type 별 파일의 수 : 볼손상 - %d, 내륜손상 - %d, 외륜손상 - %d, 정상 - %d' \n",
    "      %(len(B_files), len(IR_files), len(OR_files), len(N_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-(2). Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "각 class에 해당하는 label은 다음과 같이 one-hot으로 encoding 하겠습니다. <br/>\n",
    "볼 손상 : [1,0,0,0] / 내륜 손상 : [0,1,0,0] / 외륜 손상 : [0,0,1,0] / 정상 : [0,0,0,1] <br/>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "label = {'B': [1,0,0,0], 'IR' : [0,1,0,0], 'OR' : [0,0,1,0], 'N' : [0,0,0,1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "볼, 내륜, 외륜 손상은 파일 별로 200개의 vector가 생성되며, 정상 데이터는 파일 별로 600개의 vector가 생성됩니다. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of vectors per file\n",
    "B_vector_num = 200\n",
    "IR_vector_num = 200\n",
    "OR_vector_num = 200\n",
    "N_vector_num = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "각 data 파일 별로 min-max scaling을 적용하고, 360 points 씩 vector를 구성하도록 하겠습니다. <br/>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 볼 손상 vector generation ##\n",
    "\n",
    "B = []\n",
    "B_label = []\n",
    "\n",
    "for i in range(len(B_files)):\n",
    "    B_data = np.load('./CWRU_360_npy/' + B_files[i])\n",
    "    \n",
    "    # min-max scaling\n",
    "    B_data_scaling = B_data / (np.max(B_data) - np.min(B_data)) \n",
    "\n",
    "    \n",
    "    for j in range(0, B_vector_num):\n",
    "        B_tmp = B_data_scaling[j*360 : (j+1)*360]\n",
    "        B.append(B_tmp.tolist())\n",
    "        B_label.append(label['B'])\n",
    "        B_tmp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 내륜 손상 vector generation ##\n",
    "\n",
    "IR = []\n",
    "IR_label = []\n",
    "\n",
    "for i in range(len(IR_files)):\n",
    "    IR_data = np.load('./CWRU_360_npy/' + IR_files[i])\n",
    "    \n",
    "    # min-max scaling\n",
    "    IR_data_scaling = IR_data / (np.max(IR_data) - np.min(IR_data)) \n",
    "    \n",
    "    \n",
    "    for j in range(0, IR_vector_num):\n",
    "        IR_tmp = IR_data_scaling[j*360 : (j+1)*360]\n",
    "        IR.append(IR_tmp.tolist())\n",
    "        IR_label.append(label['IR'])\n",
    "        IR_tmp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 외륜 손상 vector generation ##\n",
    "\n",
    "OR = []\n",
    "OR_label = []\n",
    "\n",
    "for i in range(len(OR_files)):\n",
    "    OR_data = np.load('./CWRU_360_npy/' + OR_files[i])\n",
    "    \n",
    "    # min-max scaling\n",
    "    OR_data_scaling = OR_data / (np.max(OR_data) - np.min(OR_data)) \n",
    "    \n",
    "    \n",
    "    for j in range(0, OR_vector_num):\n",
    "        OR_tmp = OR_data_scaling[j*360 : (j+1)*360]\n",
    "        OR.append(OR_tmp.tolist())\n",
    "        OR_label.append(label['OR'])\n",
    "        OR_tmp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 정상 vector generation ##\n",
    "\n",
    "N = []\n",
    "N_label = []\n",
    "\n",
    "for i in range(len(N_files)):\n",
    "    N_data = np.load('./CWRU_360_npy/' + N_files[i])\n",
    "    \n",
    "    # min-max scaling\n",
    "    N_data_scaling = N_data / (np.max(N_data) - np.min(N_data)) \n",
    "    \n",
    "\n",
    "    \n",
    "    for j in range(0, N_vector_num):\n",
    "        N_tmp = N_data_scaling[j*360 : (j+1)*360]\n",
    "        N.append(N_tmp.tolist())\n",
    "        N_label.append(label['N'])\n",
    "        N_tmp = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "pre-processing이 끝난 data들을 구현상의 편의를 위해 자료형을 변환해주겠습니다.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_data = np.asarray(B)\n",
    "B_label = np.asarray(B_label)\n",
    "\n",
    "IR_data = np.asarray(IR)\n",
    "IR_label = np.asarray(IR_label)\n",
    "\n",
    "OR_data = np.asarray(OR)\n",
    "OR_label = np.asarray(OR_label)\n",
    "\n",
    "N_data = np.asarray(N)\n",
    "N_label = np.asarray(N_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "총 4개 class의 데이터들을 360 points씩 분할하여 구성한 최종 dataset은 다음과 같습니다.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img\\dataset.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "각 결함의 파형을 확인해보겠습니다.(아래 cell을 2번 실행해주셔야 그림이 보입니다.)\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fs=12000\n",
    "t_tmp = np.arange(len(B_data[0]))\n",
    "t = t_tmp/fs\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplots_adjust(hspace = 0.5, wspace = 0.3)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(t,B_data[0])\n",
    "plt.ylim(-0.5, 0.5)\n",
    "plt.title('Defect_B', fontsize=20)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(t,IR_data[0])\n",
    "plt.ylim(-0.5, 0.5)\n",
    "plt.title('Defect_IR', fontsize=20)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(t,OR_data[0])\n",
    "plt.ylim(-0.5, 0.5)\n",
    "plt.title('Defect_OR', fontsize=20)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(t,N_data[0])\n",
    "plt.ylim(-0.4, 0.4)\n",
    "plt.title('Normal', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-(3). Train / Validation / Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Model을 학습하기 위한 training set, model의 최적의 parameter를 결정하기 위한 validation set, model의 최종 성능을 평가할 test set을 구성하도록 하겠습니다.<br/>\n",
    "Class별 balance를 유지하기 위해, 각 class별로 training:test = 8:2의 비율로 구성하고, training set에서 10%는 validation set으로 구성하겠습니다.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "B_tmp, B_test, B_label_tmp, B_label_test = train_test_split(B_data, B_label, test_size = 0.2, random_state=42)\n",
    "IR_tmp, IR_test, IR_label_tmp, IR_label_test = train_test_split(IR_data, IR_label, test_size = 0.2, random_state=42)\n",
    "OR_tmp, OR_test, OR_label_tmp, OR_label_test = train_test_split(OR_data, OR_label, test_size = 0.2, random_state=42)\n",
    "N_tmp, N_test, N_label_tmp, N_label_test = train_test_split(N_data, N_label, test_size = 0.2, random_state=42)\n",
    "\n",
    "B_train, B_val, B_label_train, B_label_val = train_test_split(B_tmp, B_label_tmp, test_size = 0.1, random_state=42)\n",
    "IR_train, IR_val, IR_label_train, IR_label_val = train_test_split(IR_tmp, IR_label_tmp, test_size = 0.1, random_state=42)\n",
    "OR_train, OR_val, OR_label_train, OR_label_val = train_test_split(OR_tmp, OR_label_tmp, test_size = 0.1, random_state=42)\n",
    "N_train, N_val, N_label_train, N_label_val = train_test_split(N_tmp, N_label_tmp, test_size = 0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "각 Class별로 구성된 training, validation, test data들을 편하게 다루기 위하여 통합하겠습니다.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.concatenate((B_train, IR_train, OR_train, N_train), axis=0)\n",
    "train_label = np.concatenate((B_label_train, IR_label_train, OR_label_train, N_label_train), axis=0)\n",
    "\n",
    "val_data = np.concatenate((B_val, IR_val, OR_val, N_val), axis=0)\n",
    "val_label = np.concatenate((B_label_val, IR_label_val, OR_label_val, N_label_val), axis=0)\n",
    "\n",
    "test_data = np.concatenate((B_test, IR_test, OR_test, N_test), axis=0)\n",
    "test_label = np.concatenate((B_label_test, IR_label_test, OR_label_test, N_label_test), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Classification을 수행할 model을 생성하도록 하겠습니다.<br/>\n",
    "아래 코드는 tensor들의 통로 역할을 default graph를 초기화하는 코드로써, 여러분의 실습을 원활하게 진행하기 위한 것이므로, 추후 네트워크 구조나 hyper parameter 수정시마다 실행을 해주셔야 합니다.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-(1) Hyper parameter setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Model training에 적용할 hyper parameter들을 설정합니다.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 350\n",
    "learning_rate = 0.00001\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-(2) Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Model에 대한 input과 그에 대응하는 label을 담을 변수를 설정합니다. <br/>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_input = tf.placeholder(tf.float32, [None, 360], name='DNN_input') \n",
    "DNN_label = tf.placeholder(tf.float32, [None,4], name='DNN_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-(3) DNN model(<span style=\"color:red\">Fill in the blanks</span>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "학습을 진행할 model을 설계합니다. <br/>\n",
    "여기서는 layer별로 각각 360-150-50-4 의 neurons를 갖는 deep neural network를 사용해보겠습니다. <br/>\n",
    "첫 hidden layer에 대한 code를 참고하여, 두번째 hidden layer와 마지막 output layer를 설계해 보세요. <br/>\n",
    "(공통된 결과를 출력하기 위하여, initializer의 seed는 26으로 유지해주세요)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN(DNN_input):\n",
    "    \n",
    "   \n",
    "    ## 1st hidden layer\n",
    "    w1 = tf.get_variable(name='DNN_weight1', shape=[360, 150], initializer= tf.contrib.layers.xavier_initializer(seed=26))     \n",
    "    b1 = tf.get_variable(name='DNN_biases1', shape=[150], initializer= tf.contrib.layers.xavier_initializer(seed=26))          \n",
    "    h1 = tf.matmul(DNN_input, w1) + b1          \n",
    "    h1_relu = tf.nn.relu(h1, name='DNN_h1_relu')                \n",
    "\n",
    "\n",
    "    ## 2nd hidden layer\n",
    "    w2 = \n",
    "    b2 = \n",
    "    h2 = \n",
    "    h2_relu = \n",
    "    \n",
    "    \n",
    "    ## output layer\n",
    "    w3 = \n",
    "    b3 = \n",
    "    h3 = \n",
    "    h3_relu = \n",
    "    \n",
    "\n",
    "    return h3_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-(4) Cost, Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Model의 학습을 진행할 optimizer, 성능을 평가할 cost와 accuracy를 설정합니다.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_out = DNN(DNN_input)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=DNN_label, logits=DNN_out), name='cost')\n",
    "\n",
    "opt = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(DNN_out, 1), tf.argmax(DNN_label, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-(1) Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Model을 실행할 session을 열고, 변수들을 초기화합니다.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver\n",
    "DNN_saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "## MAKE SESSION\n",
    "DNN_sess = tf.Session()\n",
    "\n",
    "## INITIALIZE SESSION\n",
    "DNN_sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-(2) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Model을 학습하고, 매 epoch마다 validation set을 통해 model을 평가하여, 최적의 parameter들을 찾아냅니다.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_best = 0.000000001\n",
    "\n",
    "for epoch in range(n_epochs+1):\n",
    "    Z = np.random.permutation(len(train_data))\n",
    "    X_train_shuffle = train_data[Z]\n",
    "    Y_train_shuffle = train_label[Z]\n",
    "        \n",
    "    for iteration in range(len(train_data) // batch_size):\n",
    "        \n",
    "        X_batch = X_train_shuffle[iteration*batch_size : (1+iteration)*batch_size]\n",
    "        Y_batch = Y_train_shuffle[iteration*batch_size : (1+iteration)*batch_size]\n",
    "        X_batch = X_batch.reshape([-1,360])\n",
    "        Y_batch = Y_batch.reshape([-1,4])\n",
    "        \n",
    "        (_, training_cost) = DNN_sess.run([opt, cost], feed_dict={DNN_input: X_batch, DNN_label: Y_batch})\n",
    "\n",
    "    \n",
    "    ## Training accuracy every one epoch\n",
    "    acc_train = accuracy.eval(session=DNN_sess, feed_dict={DNN_input: X_batch, DNN_label: Y_batch})\n",
    "    if epoch % 1 == 0:\n",
    "        print('  [*] TRAINING Iteration %d, Loss: %.6f' % (epoch, training_cost))\n",
    "\n",
    "    ## Validation accuracy every 1 epochs\n",
    "    if epoch % 1 == 0:\n",
    "        acc_val = accuracy.eval(session=DNN_sess, feed_dict={DNN_input: val_data, DNN_label: val_label})\n",
    "        print('  [*] VALIDATION ACC: %.4f' % acc_val)\n",
    "    \n",
    "    if acc_val > last_best:\n",
    "        last_best = acc_val\n",
    "        print('*******************************')\n",
    "        DNN_saver.save(DNN_sess, './DNN.ckpt', global_step=epoch)\n",
    "        best_epoch = epoch\n",
    "\n",
    "print('Optimization done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal epoches : %d' %best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal accuracy : %f' %last_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "최적의 model에 대하여, test set을 통해 model의 최종 성능을 평가합니다.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_saver.restore(DNN_sess, './DNN.ckpt' + '-' + str(best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.eval(session=DNN_sess, feed_dict={DNN_input: test_data, DNN_label: test_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. 3-(3)의 code를 완성하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. 3-(3)에 완성된 network의 총 parameter 수를 계산하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. 3-(1)과 3-(3)의 hyper parameter들과 network 구조를 변화시켜서, accuracy를 향상시켜 보세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
